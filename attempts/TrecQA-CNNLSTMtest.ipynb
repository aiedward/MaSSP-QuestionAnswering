{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "WikiQAlstmtest.ipynb",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": [],
      "collapsed_sections": [
        "bnq3d9SuF6Fn",
        "OCQpVdU5z_4s",
        "mVQp3u8tnVI4"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "zfpNkdGUXcOT",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "outputId": "1ee44972-44ee-4818-b282-0296ee91848c",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1530764635633,
          "user_tz": -420,
          "elapsed": 14088,
          "user": {
            "displayName": "Nam Anh Dinh",
            "photoUrl": "//lh5.googleusercontent.com/-bx5rUgj7X3U/AAAAAAAAAAI/AAAAAAAAAT8/kPBiiqWYU3s/s50-c-k-no/photo.jpg",
            "userId": "105485123729788675852"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "!apt-get update -qq 2>&1 > /dev/null\n",
        "!apt-get -y install -qq google-drive-ocamlfuse fuse\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "from oauth2client.client import GoogleCredentials\n",
        "creds = GoogleCredentials.get_application_default()\n",
        "import getpass\n",
        "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "vcode = getpass.getpass()\n",
        "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\r\n",
            "··········\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "Please enter the verification code: Access token retrieved correctly.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "B-f7KbfOWJo7",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!mkdir -p drive\n",
        "!google-drive-ocamlfuse drive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Wnw6OuurfPJX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## gensim word2vec preparation"
      ]
    },
    {
      "metadata": {
        "id": "fO3gUbIpfSgG",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 758
        },
        "outputId": "e86411b1-6ff9-4ef0-de7a-9ee59e3a9027",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1530764656170,
          "user_tz": -420,
          "elapsed": 11799,
          "user": {
            "displayName": "Nam Anh Dinh",
            "photoUrl": "//lh5.googleusercontent.com/-bx5rUgj7X3U/AAAAAAAAAAI/AAAAAAAAAT8/kPBiiqWYU3s/s50-c-k-no/photo.jpg",
            "userId": "105485123729788675852"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install -U gensim\n",
        "#we use this for the word2vec representations of each word\n",
        "#remove if not using google colab. Instead, install from console"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting gensim\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/33/33/df6cb7acdcec5677ed130f4800f67509d24dbec74a03c329fcbf6b0864f0/gensim-3.4.0-cp36-cp36m-manylinux1_x86_64.whl (22.6MB)\n",
            "\u001b[K    100% |████████████████████████████████| 22.6MB 2.1MB/s \n",
            "\u001b[?25hRequirement not upgraded as not directly required: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (0.19.1)\n",
            "Collecting smart-open>=1.2.1 (from gensim)\n",
            "  Downloading https://files.pythonhosted.org/packages/cf/3d/5f3a9a296d0ba8e00e263a8dee76762076b9eb5ddc254ccaa834651c8d65/smart_open-1.6.0.tar.gz\n",
            "Requirement not upgraded as not directly required: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.11.0)\n",
            "Requirement not upgraded as not directly required: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.14.5)\n",
            "Collecting boto>=2.32 (from smart-open>=1.2.1->gensim)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bd/b7/a88a67002b1185ed9a8e8a6ef15266728c2361fcb4f1d02ea331e4c7741d/boto-2.48.0-py2.py3-none-any.whl (1.4MB)\n",
            "\u001b[K    100% |████████████████████████████████| 1.4MB 16.6MB/s \n",
            "\u001b[?25hCollecting bz2file (from smart-open>=1.2.1->gensim)\n",
            "  Downloading https://files.pythonhosted.org/packages/61/39/122222b5e85cd41c391b68a99ee296584b2a2d1d233e7ee32b4532384f2d/bz2file-0.98.tar.gz\n",
            "Requirement not upgraded as not directly required: requests in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (2.18.4)\n",
            "Collecting boto3 (from smart-open>=1.2.1->gensim)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/59/f0/22554f0fc3aafd34e189919fd6a360d440fcaa6f86dedc9aaca904c885b1/boto3-1.7.50-py2.py3-none-any.whl (128kB)\n",
            "\u001b[K    100% |████████████████████████████████| 133kB 20.6MB/s \n",
            "\u001b[?25hRequirement not upgraded as not directly required: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (3.0.4)\n",
            "Requirement not upgraded as not directly required: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (1.22)\n",
            "Requirement not upgraded as not directly required: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (2.6)\n",
            "Requirement not upgraded as not directly required: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (2018.4.16)\n",
            "Collecting jmespath<1.0.0,>=0.7.1 (from boto3->smart-open>=1.2.1->gensim)\n",
            "  Downloading https://files.pythonhosted.org/packages/b7/31/05c8d001f7f87f0f07289a5fc0fc3832e9a57f2dbd4d3b0fee70e0d51365/jmespath-0.9.3-py2.py3-none-any.whl\n",
            "Collecting botocore<1.11.0,>=1.10.50 (from boto3->smart-open>=1.2.1->gensim)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d5/9f/2e701a365b5ff0e8b664d6c393f3c61c20e52bb5148bbc2e27d737b890db/botocore-1.10.50-py2.py3-none-any.whl (4.4MB)\n",
            "\u001b[K    100% |████████████████████████████████| 4.4MB 7.1MB/s \n",
            "\u001b[?25hCollecting s3transfer<0.2.0,>=0.1.10 (from boto3->smart-open>=1.2.1->gensim)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/14/2a0004d487464d120c9fb85313a75cd3d71a7506955be458eebfe19a6b1d/s3transfer-0.1.13-py2.py3-none-any.whl (59kB)\n",
            "\u001b[K    100% |████████████████████████████████| 61kB 19.0MB/s \n",
            "\u001b[?25hRequirement not upgraded as not directly required: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.11.0,>=1.10.50->boto3->smart-open>=1.2.1->gensim) (2.5.3)\n",
            "Collecting docutils>=0.10 (from botocore<1.11.0,>=1.10.50->boto3->smart-open>=1.2.1->gensim)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/36/fa/08e9e6e0e3cbd1d362c3bbee8d01d0aedb2155c4ac112b19ef3cae8eed8d/docutils-0.14-py3-none-any.whl (543kB)\n",
            "\u001b[K    100% |████████████████████████████████| 552kB 23.3MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: smart-open, bz2file\n",
            "  Running setup.py bdist_wheel for smart-open ... \u001b[?25l-\b \b\\\b \bdone\n",
            "\u001b[?25h  Stored in directory: /content/.cache/pip/wheels/73/f1/9b/ccf93d4ba073b6f79b1ed9df68ab5ce048d8136d0efcf90b30\n",
            "  Running setup.py bdist_wheel for bz2file ... \u001b[?25l-\b \bdone\n",
            "\u001b[?25h  Stored in directory: /content/.cache/pip/wheels/81/75/d6/e1317bf09bf1af5a30befc2a007869fa6e1f516b8f7c591cb9\n",
            "Successfully built smart-open bz2file\n",
            "Installing collected packages: boto, bz2file, jmespath, docutils, botocore, s3transfer, boto3, smart-open, gensim\n",
            "Successfully installed boto-2.48.0 boto3-1.7.50 botocore-1.10.50 bz2file-0.98 docutils-0.14 gensim-3.4.0 jmespath-0.9.3 s3transfer-0.1.13 smart-open-1.6.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "zETFCLXJffvT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "use gensim to get the word2vec representations of each word  \n",
        "reference here:  \n",
        "https://www.quora.com/How-do-you-extract-vectors-from-word2vec-given-a-word"
      ]
    },
    {
      "metadata": {
        "id": "d_p6oWAvoqp0",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "from gensim.models import KeyedVectors#, Word2Vec\n",
        "\n",
        "#currently using gensim's default word2vec pretrained model, trained on googlenews data. \n",
        "#todo: train on wikiqa if the results are crap. \n",
        "\n",
        "w2vmodel = KeyedVectors.load_word2vec_format('drive/Colab Notebooks/datasets/word2vec/GoogleNews-vectors-negative300-SLIM.bin.gz', binary=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3WDQwwa4qI-y",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "word2vec first testing"
      ]
    },
    {
      "metadata": {
        "id": "QXCn6rzEqMrJ",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "f1b035fe-4e03-490c-cc26-7ab7000315e4",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1530764678621,
          "user_tz": -420,
          "elapsed": 613,
          "user": {
            "displayName": "Nam Anh Dinh",
            "photoUrl": "//lh5.googleusercontent.com/-bx5rUgj7X3U/AAAAAAAAAAI/AAAAAAAAAT8/kPBiiqWYU3s/s50-c-k-no/photo.jpg",
            "userId": "105485123729788675852"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "# print(w2vmodel.similarity('woman','man'))\n",
        "wvdict = w2vmodel.wv.vocab\n",
        "#this is the list of all the words in the vocabulary"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "HAS0ZK9HbmVO",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "734b80c1-3b0b-4e52-9501-d30194d096ed",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1530676642340,
          "user_tz": -420,
          "elapsed": 668,
          "user": {
            "displayName": "Nam Anh Dinh",
            "photoUrl": "//lh5.googleusercontent.com/-bx5rUgj7X3U/AAAAAAAAAAI/AAAAAAAAAT8/kPBiiqWYU3s/s50-c-k-no/photo.jpg",
            "userId": "105485123729788675852"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "print(w2vmodel.index2word[0])"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "in\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "6HyQR9DHduTO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Preprocessing input strings"
      ]
    },
    {
      "metadata": {
        "id": "yTkswavPBUsO",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hNJH983BqAdK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "`a.toks` contains the questions (repeat question lines correspond to the number of candidate answer sentences)  \n",
        "`b.toks` contains the answers. Each line is the answer to the question on the same line of `a.toks`  \n",
        "`id` contains the id of the question at that line in `a.toks`  \n",
        "`sim` contains the labels: for each block of repeated-questions: there is one line containing a `1` corresponding to the line in `b.toks` containing the correct answer to the same line in `a.toks`\n",
        "\n",
        "https://github.com/castorini/data/tree/master/WikiQA"
      ]
    },
    {
      "metadata": {
        "id": "rDxsDdT4W9lx",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def read_data(path, readtype):\n",
        "  with open(path) as f:\n",
        "    lines = f.readlines()\n",
        "    \n",
        "  if readtype == \"questions\" or readtype == \"answers\":\n",
        "    return lines\n",
        "  elif readtype == \"labels\":\n",
        "    return [int(line) for line in lines]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TmcQBVaWF3sk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Load the data"
      ]
    },
    {
      "metadata": {
        "id": "9o-OMzxmes6N",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "bigpath = \"drive/Colab Notebooks/datasets/QuestionAnswering/TrecQA/\"\n",
        "questions = read_data(bigpath+\"/train-all/a.toks\", \"questions\")\n",
        "answers = read_data(bigpath+\"/train-all/b.toks\", \"answers\")\n",
        "labels = read_data(bigpath+\"/train-all/sim.txt\", \"labels\")\n",
        "#question_ids = read_data(bigpath+\"/train/id.txt\", \"labels\")\n",
        "#questions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "t8v6mW3xswrR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### preparing word-level sequence embedding"
      ]
    },
    {
      "metadata": {
        "id": "cLbh0COIs4lN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This is to prepare for the Embedding layer\n",
        "\n",
        "1. concatenate questions and answers into a list of `[question | answer]` pairs. This is a list of sequences.  \n",
        "2. find max sequence length from this list  \n",
        "3. the vocab size of the google news word2vec is 3 million words, the number of features is 300\n",
        "\n",
        "The embedding_matrix:\n",
        "1. let each word in `w2vmodel.wv.vocab.keys()` have an index\n",
        "2. concatenate axis=1 all the word vectors in the vocabulary into embedding_matrix\n",
        "3. then turn each string sequence into an array of numbers (each number represents the index of that word in the vocab)\n",
        "4. pass the string sequences into the .fit as training input. The shape of this input should be (n_seqs, max_seq_length, n_features)\n",
        "5. the embedding matrix will be the weights in the Embedding layer"
      ]
    },
    {
      "metadata": {
        "id": "DH96kKR08lNS",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "vocab_size = 299567+1 #this is the SLIMmed googlenews w2v model's vocab count. #the last item is the 'all-zeroes' word vector, used for words that aren't in the w2vmodel"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mFX2hmOP64Bb",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "1774f803-2f18-4285-e5c9-04f11cb68201",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1530764693564,
          "user_tz": -420,
          "elapsed": 4008,
          "user": {
            "displayName": "Nam Anh Dinh",
            "photoUrl": "//lh5.googleusercontent.com/-bx5rUgj7X3U/AAAAAAAAAAI/AAAAAAAAAT8/kPBiiqWYU3s/s50-c-k-no/photo.jpg",
            "userId": "105485123729788675852"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "#make embedding matrix\n",
        "embedding_matrix = np.zeros((vocab_size, 300))\n",
        "for i in range(len(wvdict)):\n",
        "    embedding_vector = w2vmodel.wv[w2vmodel.wv.index2word[i]]\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "    \n",
        "  "
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "O-x4B9dDLabb",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "embedding_matrix[299566]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nORIwkiQ_S1O",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### define some functions to preprocess data (both train and test)"
      ]
    },
    {
      "metadata": {
        "id": "Hmf60uv2_bNj",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def SEQ_max_length(sequences):\n",
        "  return max( [len(qa.split()) for qa in sequences] )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EkDys4HMD4D-",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9916916d-a4fe-4432-9233-67c3a40fec04",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1530764703315,
          "user_tz": -420,
          "elapsed": 8919,
          "user": {
            "displayName": "Nam Anh Dinh",
            "photoUrl": "//lh5.googleusercontent.com/-bx5rUgj7X3U/AAAAAAAAAAI/AAAAAAAAAT8/kPBiiqWYU3s/s50-c-k-no/photo.jpg",
            "userId": "105485123729788675852"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "#from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "qp2-JrzdEVcT",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def SEQ_token_and_pad(sequences, max_seq_length):\n",
        "  \n",
        "#   #version 0a\n",
        "#   t = Tokenizer()\n",
        "#   t.fit_on_texts(sequences)\n",
        "\n",
        "#   vocab_size = len(t.word_index)+1\n",
        "\n",
        "#   encoded_seqs = t.texts_to_sequences(sequences)\n",
        "#   print(encoded_seqs)\n",
        "\n",
        "#   #version 0b and up\n",
        "  encoded_seqs = []\n",
        "  for qa in sequences:\n",
        "    encoded_qa = []\n",
        "    tokenized_qa = qa.split()\n",
        "    for word in tokenized_qa:\n",
        "      index_to_append = vocab_size - 1\n",
        "      \n",
        "      if word in wvdict.keys():\n",
        "        index_to_append = w2vmodel.vocab[word].index\n",
        "      encoded_qa.append(index_to_append)\n",
        "    \n",
        "    encoded_seqs.append(encoded_qa)\n",
        "    \n",
        "  #print(encoded_seqs)\n",
        "  \n",
        "  padded_seqs = pad_sequences(encoded_seqs, maxlen = max_seq_length, padding=\"post\", value = vocab_size - 1)\n",
        "  return padded_seqs\n",
        "\n",
        "#version 0\n",
        "#padded_seqs = SEQ_token_and_pad(sequences, max_seq_length)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WR3lQtUY7BCC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Performing the necessary preprocessing on our data using our functions"
      ]
    },
    {
      "metadata": {
        "id": "UTbIE6T37AQl",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "max_question_length = SEQ_max_length(questions)\n",
        "max_answer_length = SEQ_max_length(answers)\n",
        "max_hidden_length = max(max_question_length, max_answer_length)\n",
        "\n",
        "#version 1: getting separate questions and answers vectors (but still encoded in index form, not in pure word vector form)\n",
        "padded_questions = SEQ_token_and_pad(questions, max_hidden_length)\n",
        "padded_answers = SEQ_token_and_pad(answers, max_hidden_length)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aFpPvwGB7Z4A",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "outputId": "86556977-49f2-4dfe-bbb1-cabc2c3b8649",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1530690134720,
          "user_tz": -420,
          "elapsed": 677,
          "user": {
            "displayName": "Nam Anh Dinh",
            "photoUrl": "//lh5.googleusercontent.com/-bx5rUgj7X3U/AAAAAAAAAAI/AAAAAAAAAT8/kPBiiqWYU3s/s50-c-k-no/photo.jpg",
            "userId": "105485123729788675852"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "#rebuild a question and answer from the padded representation\n",
        "print(padded_questions[0])\n",
        "print(max_hidden_length)\n",
        "\n",
        "\" \".join([w2vmodel.wv.index2word[index] for index in padded_answers[0] if not index == 299567])\n",
        "\n"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[   132     17  27136  22292   2874 299567 299567 299567 299567 299567\n",
            " 299567 299567 299567 299567 299567 299567 299567 299567 299567 299567\n",
            " 299567 299567 299567 299567 299567 299567 299567 299567 299567 299567\n",
            " 299567 299567 299567 299567 299567 299567 299567 299567 299567 299567]\n",
            "40\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
            "  after removing the cwd from sys.path.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'partly submerged glacier cave on moreno glacier'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "metadata": {
        "id": "FXnePrC9PGrb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### creating one-hot labels"
      ]
    },
    {
      "metadata": {
        "id": "Xg2CQ9NRPKuL",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OneHotEncoder"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gLedyG96PQct",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "labels_np = np.array(labels)\n",
        "labels_np = np.expand_dims(labels_np, axis=1)\n",
        "\n",
        "onehot = OneHotEncoder(n_values = 2)\n",
        "onehot.fit(labels_np)\n",
        "labels_np = onehot.transform(labels_np)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Oc-VoTmpsdmc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Model building and training"
      ]
    },
    {
      "metadata": {
        "id": "C4nQNUEvjSqP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Guesswork and experimentation"
      ]
    },
    {
      "metadata": {
        "id": "y9M2Frrx0Wrz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "*If i'm doing the implementation in the 1506.06490 paper*  \n",
        "**Todo**: \n",
        "\n",
        "- find length of the longest question and longest answer\n",
        "- use these maxes for padding to shorter sentences\n",
        "- in one big model, build 2 separate convnet submodels, one for questions, one for answers\n",
        "- concatenate the outputs for the questions and answers convnets, and we obtain the **joint representation** `p` of each question-answer pair.\n",
        "\n",
        "Each of these learned joint pairs is a single **timestep** in the LSTM part. We learn the sequence and semantic relationships between each answer-candidate sentence in the context paragraph, not between the words themselves. (the word-level patterns are learned in the Conv stage already)\n",
        "\n",
        "for the LSTM stage, padding instead involves adding dummy qa pairs for each timestep without an answer. The max number of answer candidates in the dataset (and possible qa pairs, per question) is 30, so any Qs with fewer than 30 answer sentences will be padded with dummy pairs.\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "DnGBC-R5EVCt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "*my own simple-ass idea*  \n",
        "**Todo:**\n",
        "\n",
        "version 0a: `shit lstm model copped from some blog`  \n",
        "https://machinelearningmastery.com/sequence-classification-lstm-recurrent-neural-networks-python-keras/\n",
        "\n",
        "version 0b: `with learning rate, and fixed word tokenizations`\n",
        "\n",
        "version 1a: `separate LSTM for question and answer, then Concatenate into a hidden layer which then is fed into classifier/Dense/whatever`\n",
        "\n",
        "version 1b: `no LSTM for question and answer. Using Functional API. WORKS but OVERFITS.`"
      ]
    },
    {
      "metadata": {
        "id": "SgNENhwKWZ5A",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Giang's idea"
      ]
    },
    {
      "metadata": {
        "id": "sdsWQTH8S57x",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Giang's idea:\n",
        "\n",
        "`doc question  \n",
        "input_1 = Input(...)  \n",
        "...  \n",
        "output_1 = ...\n",
        "\n",
        "\n",
        "doc answer  \n",
        "input_2 = Input(...)  \n",
        "...  \n",
        "output_2 = ...\n",
        "\n",
        "input_2 = Embedding(vocab_size, 300, weights = [embedding_matrix], input_length = max_answer_length, trainable=False)  \n",
        "  \n",
        "output_2 = LSTM(100)(input_reshape_2)\n",
        "\n",
        "hidden = Concatenate()([output_1, output_2])\n",
        "classifier = Dense(1, activation=\"sigmoid\")(hidden)\n",
        "`"
      ]
    },
    {
      "metadata": {
        "id": "2gndJjO6SxsM",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# #not using functional API\n",
        "# #dont use this, non working\n",
        "\n",
        "# questions_model = Sequential()\n",
        "# questions_model.add(equestions)\n",
        "# #questions_model.add(LSTM(64))\n",
        "# # questions_model.add(Activation('softmax'))\n",
        "# #questions_model.add(Dropout(0.4))\n",
        "\n",
        "# answers_model = Sequential()\n",
        "# answers_model.add(eanswers)\n",
        "# #answers_model.add(LSTM(64))\n",
        "# # answers_model.add(Activation('softmax'))\n",
        "# #answers_model.add(Dropout(0.4))\n",
        "\n",
        "# merged = Merge([questions_model.output, answers_model.output], mode = \"concat\")\n",
        "# print(merged.output.shape)\n",
        "# final_model = Sequential()\n",
        "# final_model.add(merged)\n",
        "# #final_model.add(Flatten())\n",
        "# final_model.add(LSTM(100)) #comment out if using lstm on the question and answer submodels\n",
        "# final_model.add(Dense(2, activation=\"softmax\"))\n",
        "\n",
        "# final_model.compile(loss = 'categorical_crossentropy', optimizer = Adam(), metrics=['accuracy'])\n",
        "# print(final_model.summary())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_kxBIRzPT1Fd",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 650
        },
        "outputId": "b2cd2139-eca9-4560-9f0a-fe61ada83e90",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1530765059037,
          "user_tz": -420,
          "elapsed": 3248,
          "user": {
            "displayName": "Nam Anh Dinh",
            "photoUrl": "//lh5.googleusercontent.com/-bx5rUgj7X3U/AAAAAAAAAAI/AAAAAAAAAT8/kPBiiqWYU3s/s50-c-k-no/photo.jpg",
            "userId": "105485123729788675852"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import Input, Embedding, Concatenate, LSTM, Dense, Conv1D, MaxPooling1D, Dropout\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "#using keras functional api\n",
        "\n",
        "qumodel_input = Input(shape = (max_hidden_length,), name=\"questionsMainInput\")\n",
        "qumodel_embed = Embedding(vocab_size, 300, weights=[embedding_matrix], input_length=max_hidden_length, trainable=False)(qumodel_input)\n",
        "qumodel_parse = Conv1D(32, 2)(qumodel_embed)\n",
        "qumodel_parse = Dropout(0.2)(qumodel_parse)\n",
        "qumodel_parse = MaxPooling1D(pool_size = 2)(qumodel_parse)\n",
        "\n",
        "ansmodel_input = Input(shape = (max_hidden_length,), name=\"answersMainInput\")\n",
        "ansmodel_embed = Embedding(vocab_size, 300, weights=[embedding_matrix], input_length=max_hidden_length, trainable=False)(ansmodel_input)\n",
        "ansmodel_parse = Conv1D(32,2)(ansmodel_embed)\n",
        "ansmodel_parse = Dropout(0.2)(ansmodel_parse)\n",
        "ansmodel_parse = MaxPooling1D(pool_size = 2)(ansmodel_parse)\n",
        "\n",
        "combinedmodel = Concatenate()([qumodel_parse, ansmodel_parse])\n",
        "print(combinedmodel.shape)\n",
        "\n",
        "lstmcombinedmodel = LSTM(64)(combinedmodel)\n",
        "classifier = Dense(2, activation='softmax')(lstmcombinedmodel)\n",
        "\n",
        "final_model = Model(inputs = [qumodel_input, ansmodel_input], outputs = classifier)\n",
        "\n",
        "final_model.compile(loss = 'categorical_crossentropy', optimizer = Adam(lr = 0.003), metrics=['accuracy'])\n",
        "print(final_model.summary())"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(?, 29, 64)\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "questionsMainInput (InputLayer) (None, 60)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "answersMainInput (InputLayer)   (None, 60)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_1 (Embedding)         (None, 60, 300)      89870400    questionsMainInput[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "embedding_2 (Embedding)         (None, 60, 300)      89870400    answersMainInput[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1 (Conv1D)               (None, 59, 32)       19232       embedding_1[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_2 (Conv1D)               (None, 59, 32)       19232       embedding_2[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 59, 32)       0           conv1d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_2 (Dropout)             (None, 59, 32)       0           conv1d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_1 (MaxPooling1D)  (None, 29, 32)       0           dropout_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_2 (MaxPooling1D)  (None, 29, 32)       0           dropout_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 29, 64)       0           max_pooling1d_1[0][0]            \n",
            "                                                                 max_pooling1d_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   (None, 64)           33024       concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 2)            130         lstm_1[0][0]                     \n",
            "==================================================================================================\n",
            "Total params: 179,812,418\n",
            "Trainable params: 71,618\n",
            "Non-trainable params: 179,740,800\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "OCQpVdU5z_4s",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Attempting the 1506 06490 paper"
      ]
    },
    {
      "metadata": {
        "id": "bcHB2znf0GFZ",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "outputId": "5a251758-487e-46b6-bc5d-f0c99413d260",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1530691960793,
          "user_tz": -420,
          "elapsed": 631,
          "user": {
            "displayName": "Nam Anh Dinh",
            "photoUrl": "//lh5.googleusercontent.com/-bx5rUgj7X3U/AAAAAAAAAAI/AAAAAAAAAT8/kPBiiqWYU3s/s50-c-k-no/photo.jpg",
            "userId": "105485123729788675852"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "from keras.layers import Dense, LSTM, Conv1D, Embedding, MaxPooling1D, Activation, Concatenate\n",
        "from keras.models import Model\n",
        "from keras.optimizers import Adadelta"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zI1Wh0qCz7SQ",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 315
        },
        "outputId": "ac2394c2-51df-4a2f-965d-879837ed5e63",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1530692147421,
          "user_tz": -420,
          "elapsed": 4181,
          "user": {
            "displayName": "Nam Anh Dinh",
            "photoUrl": "//lh5.googleusercontent.com/-bx5rUgj7X3U/AAAAAAAAAAI/AAAAAAAAAT8/kPBiiqWYU3s/s50-c-k-no/photo.jpg",
            "userId": "105485123729788675852"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "#embedding is Embedding(vocab_size, 300, weights = [embedding_matrix], input_length = max_hidden_length, trainable = False)\n",
        "\n",
        "questions_model = Sequential()\n",
        "questions_model.add(Embedding(vocab_size, 300, weights = [embedding_matrix], input_length = max_hidden_length, trainable = False))\n",
        "questions_model.add(Conv1D(100, 5, strides=1))\n",
        "questions_model.add(MaxPooling1D(pool_size=2))\n",
        "questions_model.add(Activation('relu'))\n",
        "\n",
        "questions_model.add(Conv1D(100, 3, strides=1))\n",
        "questions_model.add(MaxPooling1D(pool_size=2))\n",
        "questions_model.add(Activation('relu'))\n",
        "\n",
        "questions_model.add(Conv1D(100, 3, strides=1))\n",
        "questions_model.add(MaxPooling1D(pool_size=1))\n",
        "questions_model.add(Activation('relu'))\n",
        "\n",
        "answers_model = Sequential()\n",
        "answers_model.add(Embedding(vocab_size, 300, weights = [embedding_matrix], input_length = max_hidden_length, trainable = False))\n",
        "answers_model.add(Conv1D(100, 5, strides=1))\n",
        "answers_model.add(MaxPooling1D(pool_size=2))\n",
        "answers_model.add(Activation('relu'))\n",
        "\n",
        "answers_model.add(Conv1D(100, 3, strides=1))\n",
        "answers_model.add(MaxPooling1D(pool_size=2))\n",
        "answers_model.add(Activation('relu'))\n",
        "\n",
        "answers_model.add(Conv1D(100, 3, strides=1))\n",
        "answers_model.add(MaxPooling1D(pool_size=1))\n",
        "answers_model.add(Activation('relu'))\n",
        "\n",
        "merge = Merge([questions_model, answers_model])\n",
        "\n",
        "print(merge.output.shape)\n",
        "\n",
        "final_model = Sequential()\n",
        "final_model.add(merge)\n",
        "final_model.add(Flatten())\n",
        "# final_model.add(LSTM(160))\n",
        "final_model.add(Dense(1, activation=\"softmax\"))\n",
        "\n",
        "final_model.compile(loss = 'binary_crossentropy', optimizer = Adadelta(lr=0.5), metrics=['accuracy'])\n",
        "print(final_model.summary())\n"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:30: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(?, 6, 100)\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "merge_6 (Merge)              (None, 6, 100)            0         \n",
            "_________________________________________________________________\n",
            "flatten_3 (Flatten)          (None, 600)               0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1)                 601       \n",
            "=================================================================\n",
            "Total params: 180,162,001\n",
            "Trainable params: 421,201\n",
            "Non-trainable params: 179,740,800\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "7oX-EOamPWYp",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 1363
        },
        "outputId": "bd715b78-3804-4aa4-89e4-a02833c9b8df",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1530758986539,
          "user_tz": -420,
          "elapsed": 7966,
          "user": {
            "displayName": "Nam Anh Dinh",
            "photoUrl": "//lh5.googleusercontent.com/-bx5rUgj7X3U/AAAAAAAAAAI/AAAAAAAAAT8/kPBiiqWYU3s/s50-c-k-no/photo.jpg",
            "userId": "105485123729788675852"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import Input, Embedding, Concatenate, LSTM, Dense, Conv1D, MaxPooling1D, Dropout, Activation\n",
        "from keras.optimizers import Adadelta\n",
        "from keras.regularizers import l1\n",
        "\n",
        "#using keras functional api\n",
        "cnns = [5, 2, 2]\n",
        "pools = [2, 2, 1]\n",
        "\n",
        "qumodel_input = Input(shape = (max_hidden_length,), name=\"questionsMainInput\")\n",
        "qumodel_parse = Embedding(vocab_size, 300, weights=[embedding_matrix], input_length=max_hidden_length, trainable=False)(qumodel_input)\n",
        "for layer in range(len(cnns)):\n",
        "  qumodel_parse = Conv1D(100, cnns[layer])(qumodel_parse)\n",
        "  qumodel_parse = Dropout(0.2)(qumodel_parse)\n",
        "  qumodel_parse = MaxPooling1D(pool_size = pools[layer])(qumodel_parse)\n",
        "  qumodel_parse = Activation('relu')(qumodel_parse)\n",
        "\n",
        "print(qumodel_parse.shape)\n",
        "\n",
        "ansmodel_input = Input(shape = (max_hidden_length,), name=\"answersMainInput\")\n",
        "ansmodel_parse = Embedding(vocab_size, 300, weights=[embedding_matrix], input_length=max_hidden_length, trainable=False)(ansmodel_input)\n",
        "for layer in range(len(cnns)):\n",
        "  ansmodel_parse = Conv1D(100, cnns[layer])(ansmodel_parse)\n",
        "  ansmodel_parse = Dropout(0.2)(ansmodel_parse)\n",
        "  ansmodel_parse = MaxPooling1D(pool_size = pools[layer])(ansmodel_parse)\n",
        "  ansmodel_parse = Activation('relu')(ansmodel_parse)\n",
        "  \n",
        "print(ansmodel_parse.shape)\n",
        "\n",
        "combinedmodel = Concatenate()([qumodel_parse, ansmodel_parse])\n",
        "print(combinedmodel.shape)\n",
        "\n",
        "lstmcombinedmodel = LSTM(100)(combinedmodel)\n",
        "classifier = Dense(2, activation='softmax')(lstmcombinedmodel)\n",
        "\n",
        "final_model = Model(inputs = [qumodel_input, ansmodel_input], outputs = classifier)\n",
        "\n",
        "final_model.compile(loss = 'categorical_crossentropy', optimizer = Adadelta(lr = 0.01), metrics=['accuracy'])\n",
        "print(final_model.summary())"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(?, 7, 100)\n",
            "(?, 7, 100)\n",
            "(?, 7, 200)\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "questionsMainInput (InputLayer) (None, 40)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "answersMainInput (InputLayer)   (None, 40)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_20 (Embedding)        (None, 40, 300)      89870400    questionsMainInput[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "embedding_21 (Embedding)        (None, 40, 300)      89870400    answersMainInput[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_37 (Conv1D)              (None, 36, 100)      150100      embedding_20[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_40 (Conv1D)              (None, 36, 100)      150100      embedding_21[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dropout_37 (Dropout)            (None, 36, 100)      0           conv1d_37[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_40 (Dropout)            (None, 36, 100)      0           conv1d_40[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_37 (MaxPooling1D) (None, 18, 100)      0           dropout_37[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_40 (MaxPooling1D) (None, 18, 100)      0           dropout_40[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_37 (Activation)      (None, 18, 100)      0           max_pooling1d_37[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "activation_40 (Activation)      (None, 18, 100)      0           max_pooling1d_40[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_38 (Conv1D)              (None, 17, 100)      20100       activation_37[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_41 (Conv1D)              (None, 17, 100)      20100       activation_40[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_38 (Dropout)            (None, 17, 100)      0           conv1d_38[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_41 (Dropout)            (None, 17, 100)      0           conv1d_41[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_38 (MaxPooling1D) (None, 8, 100)       0           dropout_38[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_41 (MaxPooling1D) (None, 8, 100)       0           dropout_41[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_38 (Activation)      (None, 8, 100)       0           max_pooling1d_38[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "activation_41 (Activation)      (None, 8, 100)       0           max_pooling1d_41[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_39 (Conv1D)              (None, 7, 100)       20100       activation_38[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_42 (Conv1D)              (None, 7, 100)       20100       activation_41[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_39 (Dropout)            (None, 7, 100)       0           conv1d_39[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_42 (Dropout)            (None, 7, 100)       0           conv1d_42[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_39 (MaxPooling1D) (None, 7, 100)       0           dropout_39[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_42 (MaxPooling1D) (None, 7, 100)       0           dropout_42[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_39 (Activation)      (None, 7, 100)       0           max_pooling1d_39[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "activation_42 (Activation)      (None, 7, 100)       0           max_pooling1d_42[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_10 (Concatenate)    (None, 7, 200)       0           activation_39[0][0]              \n",
            "                                                                 activation_42[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "lstm_11 (LSTM)                  (None, 100)          120400      concatenate_10[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dense_7 (Dense)                 (None, 2)            202         lstm_11[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 180,242,002\n",
            "Trainable params: 501,202\n",
            "Non-trainable params: 179,740,800\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "mVQp3u8tnVI4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Attempting the dl-models-for-qa QA-LSTM model"
      ]
    },
    {
      "metadata": {
        "id": "luI-4rvinjtC",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 263
        },
        "outputId": "817dddf5-c129-4323-8bf9-10e51a0665ab",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1530690940644,
          "user_tz": -420,
          "elapsed": 5817,
          "user": {
            "displayName": "Nam Anh Dinh",
            "photoUrl": "//lh5.googleusercontent.com/-bx5rUgj7X3U/AAAAAAAAAAI/AAAAAAAAAT8/kPBiiqWYU3s/s50-c-k-no/photo.jpg",
            "userId": "105485123729788675852"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, Bidirectional, LSTM, Dropout, Merge, Dense\n",
        "WORD2VEC_EMBED_SIZE = 300\n",
        "QA_EMBED_SIZE = 64\n",
        "BATCH_SIZE = 64\n",
        "NBR_EPOCHS = 10\n",
        "\n",
        "qenc = Sequential()\n",
        "qenc.add(Embedding(output_dim=WORD2VEC_EMBED_SIZE, input_dim=vocab_size,\n",
        "                   weights=[embedding_matrix], mask_zero=True))\n",
        "qenc.add(Bidirectional(LSTM(QA_EMBED_SIZE, input_shape=(None, 300, max_hidden_length), \n",
        "                            return_sequences=False), merge_mode=\"sum\"))\n",
        "qenc.add(Dropout(0.3))\n",
        "\n",
        "aenc = Sequential()\n",
        "aenc.add(Embedding(output_dim=WORD2VEC_EMBED_SIZE, input_dim=vocab_size,\n",
        "                   weights=[embedding_matrix], mask_zero=True))\n",
        "aenc.add(Bidirectional(LSTM(QA_EMBED_SIZE, input_shape=(None, 300, max_hidden_length), \n",
        "                            return_sequences=False), merge_mode=\"sum\"))\n",
        "aenc.add(Dropout(0.3))\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Merge([qenc, aenc], mode=\"sum\"))\n",
        "model.add(Dense(1, activation=\"softmax\"))\n",
        "\n",
        "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "print(model.summary())\n"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "merge_6 (Merge)              (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 180,114,625\n",
            "Trainable params: 180,114,625\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:23: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "Lf7liswTpRX_",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hfOWBI3J7s8N",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Fit the chosen model"
      ]
    },
    {
      "metadata": {
        "id": "SgxZXbGTFVRC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Severely punish \"no\" answers because that's just lazy. The network should at least try to guess \"yes\" or \"1\" on at least some questions, and not settle for a 88% accuracy by answering no to everything.\n",
        "\n",
        "add class_weights and try:"
      ]
    },
    {
      "metadata": {
        "id": "95zZqZ8aFG6d",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "outputId": "1a8580b6-797e-4527-dbeb-3aba5f63edbd",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1530756603394,
          "user_tz": -420,
          "elapsed": 780,
          "user": {
            "displayName": "Nam Anh Dinh",
            "photoUrl": "//lh5.googleusercontent.com/-bx5rUgj7X3U/AAAAAAAAAAI/AAAAAAAAAT8/kPBiiqWYU3s/s50-c-k-no/photo.jpg",
            "userId": "105485123729788675852"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "from sklearn.utils import class_weight\n",
        "\n",
        "class_weights = class_weight.compute_class_weight('balanced', np.unique(labels), labels)\n",
        "#class_weights = np.array([0.5, 24.])"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Kd34uHOvMRE1",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 1507
        },
        "outputId": "9414eddd-6701-468e-8c65-e17b4f918d6c",
        "executionInfo": {
          "status": "error",
          "timestamp": 1530765343059,
          "user_tz": -420,
          "elapsed": 258782,
          "user": {
            "displayName": "Nam Anh Dinh",
            "photoUrl": "//lh5.googleusercontent.com/-bx5rUgj7X3U/AAAAAAAAAAI/AAAAAAAAAT8/kPBiiqWYU3s/s50-c-k-no/photo.jpg",
            "userId": "105485123729788675852"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "#final_model.fit([padded_questions, padded_answers], labels, epochs=5, batch_size=128, verbose=1, class_weight = class_weights, validation_split=0.2)\n",
        "\n",
        "#model.fit([padded_questions, padded_answers], labels, batch_size=32,\n",
        " #         epochs=20, validation_split=0.1, class_weight = class_weights)\n",
        "  \n",
        "final_model.fit([padded_questions, padded_answers], labels_np, epochs=30, batch_size=64, verbose=1, validation_split=0.1)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 48075 samples, validate on 5342 samples\n",
            "Epoch 1/30\n",
            "44352/48075 [==========================>...] - ETA: 4s - loss: 0.3436 - acc: 0.8782"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "48075/48075 [==============================] - 64s 1ms/step - loss: 0.3410 - acc: 0.8787 - val_loss: 0.3463 - val_acc: 0.8843\n",
            "Epoch 2/30\n",
            "27456/48075 [================>.............] - ETA: 25s - loss: 0.3006 - acc: 0.8839"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "48075/48075 [==============================] - 62s 1ms/step - loss: 0.2962 - acc: 0.8852 - val_loss: 0.3633 - val_acc: 0.8811\n",
            "Epoch 3/30\n",
            "21056/48075 [============>.................] - ETA: 34s - loss: 0.2716 - acc: 0.8921"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "48075/48075 [==============================] - 62s 1ms/step - loss: 0.2674 - acc: 0.8946 - val_loss: 0.3501 - val_acc: 0.8778\n",
            "Epoch 4/30\n",
            "18624/48075 [==========>...................] - ETA: 37s - loss: 0.2488 - acc: 0.9013"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "48075/48075 [==============================] - 62s 1ms/step - loss: 0.2476 - acc: 0.9023 - val_loss: 0.3492 - val_acc: 0.8759\n",
            "Epoch 5/30\n",
            " 5056/48075 [==>...........................] - ETA: 52s - loss: 0.2377 - acc: 0.9029"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-b404a655cf28>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfinal_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpadded_questions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadded_answers\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_np\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1703\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1704\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1705\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m     def evaluate(self, x=None, y=None,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1234\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1236\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1237\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2480\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2481\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2482\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2483\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1135\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1136\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1316\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1317\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1307\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m           run_metadata)\n\u001b[0m\u001b[1;32m   1410\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "ZslLPJeSN1a1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Testing the trained model"
      ]
    },
    {
      "metadata": {
        "id": "9TfGtdtoOdbw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Preparing the test data"
      ]
    },
    {
      "metadata": {
        "id": "RKf_rtGrN4fO",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "test_questions = [\"Where was Tuan born\",\"Where was Tuan born\",\"Where was Tuan born\",\"Where was Tuan born\"]\n",
        "test_answers = [\"Pigs dogs cats\",\"Boo\",\"Candy balls cake\", \"Tuan was born in Vietnam\"]\n",
        "test_labels = [0, 0, 0, 1]\n",
        "\n",
        "\n",
        "\n",
        "# test_questions = read_data(bigpath+\"/test/a.toks\", \"questions\")\n",
        "# test_answers = read_data(bigpath+\"/test/b.toks\", \"answers\")\n",
        "# test_labels = read_data(bigpath+\"/test/sim.txt\", \"labels\")\n",
        "# question_ids = read_data(bigpath+\"/dev/id.txt\", \"labels\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7cclUTguOx0e",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "#test_seqs, max_test_seq_length = SEQ_make_qa_sequences_list(test_questions, test_answers)\n",
        "padded_test_questions = SEQ_token_and_pad(test_questions, max_hidden_length)\n",
        "padded_test_answers = SEQ_token_and_pad(test_answers, max_hidden_length)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XX-Hb8hvPPTU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Predicting some q, a pairs"
      ]
    },
    {
      "metadata": {
        "id": "u1oZhJXePMLL",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "test_preds = final_model.predict_on_batch([padded_test_questions, padded_test_answers])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8VAqYQZ-YWTe",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "6bf26b0e-1833-4940-bb8e-80f30869e327",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1530761445690,
          "user_tz": -420,
          "elapsed": 907,
          "user": {
            "displayName": "Nam Anh Dinh",
            "photoUrl": "//lh5.googleusercontent.com/-bx5rUgj7X3U/AAAAAAAAAAI/AAAAAAAAAT8/kPBiiqWYU3s/s50-c-k-no/photo.jpg",
            "userId": "105485123729788675852"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "print(test_preds)\n",
        "np.argmax(test_preds*1000)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[9.9997854e-01 2.1408780e-05]\n",
            " [9.9997807e-01 2.1898602e-05]\n",
            " [9.9996340e-01 3.6599067e-05]\n",
            " [9.9997723e-01 2.2740214e-05]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "metadata": {
        "id": "E3NEhF84Rgnq",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "c7ace5aa-a5e5-44cf-a0b7-2526d8add1b5",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1530760347063,
          "user_tz": -420,
          "elapsed": 672,
          "user": {
            "displayName": "Nam Anh Dinh",
            "photoUrl": "//lh5.googleusercontent.com/-bx5rUgj7X3U/AAAAAAAAAAI/AAAAAAAAAT8/kPBiiqWYU3s/s50-c-k-no/photo.jpg",
            "userId": "105485123729788675852"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "i=4 #test example to check\n",
        "\n",
        "# print(padded_test_seqs[1])\n",
        "# print(padded_test_seqs[16])\n",
        "\n",
        "print(test_preds[i])\n",
        "print(test_questions[i])\n",
        "print(test_answers[i])\n",
        "print(\"correct label: \"+str(test_labels[i]))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[9.9986219e-01 1.3773878e-04]\n",
            "how are glacier caves formed ?\n",
            "\n",
            "glacier caves are often called ice caves , but this term is properly used to describe bedrock caves that contain year-round ice .\n",
            "\n",
            "correct label: 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "TQlY2QuGWDsL",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}